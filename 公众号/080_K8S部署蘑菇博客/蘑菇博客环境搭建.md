



# kubernetes搭建蘑菇博客项目



## 一.安装部署k8s集群

### 1.初始化集群系统

```shell
1.初始化集群信息
# 关闭防火墙
systemctl stop firewalld && systemctl disable firewalld && systemctl status firewalld
# 关闭selinux
setenforce 0
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
# 配置主机网卡信息
cp /etc/sysconfig/network-scripts/ifcfg-ens33{,.bak}
cat > /etc/sysconfig/network-scripts/ifcfg-ens33<<EOF
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=static
IPADDR=192.168.229.5
NETMASK=255.255.255.0
GATEWAY=192.168.229.2
DNS1=192.168.229.2
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=ens33
DEVICE=ens33
ONBOOT=yes
EOF
# 重启网络
systemctl restart network
# 修改主机名称
hostnamectl set-hostname k8s-master1 && bash 
# 关闭swap 分区
swapoff -a
sed -ri 's/.*swap.*/#&/' /etc/fstab
# 配置hosts文件
cat >> /etc/hosts<<EOF
192.168.229.3   k8s-master1
192.168.229.4   k8s-master2
EOF
# 配置内核参数
modprobe br_netfilter
echo "modprobe br_netfilter" >> /etc/profile
cat > /etc/sysctl.d/k8s.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
sysctl -p /etc/sysctl.d/k8s.conf

# 安装rpm 依赖包
yum -y install vim lrzsz unzip wget
cd  ./k8s-yilai/k8s-rpm  &&  rpm -Uvh *.rpm  --force --nodeps  && cd ../docker &&  rpm -Uvh *.rpm  --force --nodeps  

# 配置本地源
mkdir -p /root/yum  && mv /etc/yum.repos.d/* /root/yum
cp  /root/k8s-yilai/{CentOS-Base.repo,epel.repo}  /etc/yum.repos.d/
# 同步时间
echo "* */1 * * * /usr/sbin/ntpdate   cn.pool.ntp.org" >> /var/spool/cron/root
systemctl restart crond
# 开启ipvs模式
cp  /root/k8s-yilai/ipvs.modules  /etc/sysconfig/modules/
chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep ip_vs
#禁用iptables清空防火墙规则
service iptables stop   && systemctl disable iptables && systemctl status iptables && iptables -F


```



### 2.安装docker及所需rpm包，以及harbor仓库

```shell
# 启动docker 
systemctl start docker && systemctl enable docker.service && systemctl status docker
cat >  /etc/docker/daemon.json <<EOF
{
"registry-mirrors":["https://rsbud4vc.mirror.aliyuncs.com","https://registry.docker-cn.com","https://docker.mirrors.ustc.edu.cn","https://dockerhub.azk8s.cn","http://hub-mirror.c.163.com","http://qtid6917.mirror.aliyuncs.com"], 
"exec-opts":["native.cgroupdriver=systemd"],
 "log-driver":"json-file",
 "log-opts": {
  "max-size": "100m"
  },
 "storage-driver":"overlay2",
 "storage-opts": [
  "overlay2.override_kernel_check=true"
  ]
}
EOF
systemctl daemon-reload  && systemctl restart docker && systemctl status docker
```



### 3.安装kubeadm - 1.20版本

```shell
# 安装k8s 1.20 版本 （master1，node1，node2节点执行）
cd /root/k8s-yilai/k8s-1.20-rpm/ && rpm -Uvh *.rpm --force --nodeps
systemctl enable kubelet && systemctl start kubelet  && systemctl status kubelet
cd /root/k8s-yilai && docker load -i k8simage-1-20-6.tar.gz

```





### 4.初始化k8s集群，加入master节点和node节点

```shell
1.使用kubeadm初始化k8s集群
[root@k8s-master1 nginx]#  kubeadm init --apiserver-advertise-address=192.168.229.3 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.20.6 --service-cidr=10.96.0.0/12  --pod-network-cidr=10.244.0.0/16

注：--image-repository registry.aliyuncs.com/google_containers：手动指定仓库地址为registry.aliyuncs.com/google_containers。kubeadm默认从k8s.grc.io拉取镜像，但是k8s.gcr.io访问不到，所以需要指定从registry.aliyuncs.com/google_containers仓库拉取镜像。

# 输出如下：
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

# node节点加入
kubeadm join 192.168.229.3:6443 --token gxm5he.0y4iswzsm1x8gf2j \
    --discovery-token-ca-cert-hash sha256:773f83257552729023635bef974f71212979a9a3fb34a5813dc5478bdd65e5ad 


# ps 如安装失败
kubeadm reset    
rm -rf  /etc/kubernetes/
rm -rf /var/lib/etcd/

[root@k8s-master1 ~]# scp -r  .kube/  192.168.229.4:/root/

[root@k8s-master1 ~]# kubectl get node
NAME          STATUS     ROLES                  AGE     VERSION
k8s-master1   NotReady   control-plane,master   2m22s   v1.20.6
k8s-node1     NotReady   <none>                 103s    v1.20.6

#  到这里k8s 集群就已经安装完成了

2.打上node工作节点worker标签
[root@k8s-master1 k8s-yilai]# kubectl label node k8s-node1 node-role.kubernetes.io/worker=worker

3. 安装kubernetes网络组件-Calico （master1节点执行）
[root@k8s-master1 k8s-yilai]# kubectl apply -f  /root/k8s-yilai/calico.yaml
注：在线下载配置文件地址是： https://docs.projectcalico.org/manifests/calico.yaml


[root@k8s-master1 k8s-yilai]# kubectl get node
NAME          STATUS   ROLES                  AGE     VERSION
k8s-master1   Ready    control-plane,master   5m18s   v1.20.6
k8s-node1     Ready    worker                 4m39s   v1.20.6
[root@k8s-master1 k8s-yilai]# kubectl get po -A 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-6949477b58-2fnj2   1/1     Running   0          25s
kube-system   calico-node-9s7n8                          1/1     Running   0          25s
kube-system   calico-node-kc2f9                          1/1     Running   0          25s
kube-system   coredns-7f89b7bc75-cm9bx                   1/1     Running   0          5m5s
kube-system   coredns-7f89b7bc75-j86ff                   1/1     Running   0          5m5s
kube-system   etcd-k8s-master1                           1/1     Running   0          5m20s
kube-system   kube-apiserver-k8s-master1                 1/1     Running   0          5m20s
kube-system   kube-controller-manager-k8s-master1        1/1     Running   0          5m20s
kube-system   kube-proxy-4ft9w                           1/1     Running   0          5m5s
kube-system   kube-proxy-cflgh                           1/1     Running   0          4m43s
kube-system   kube-scheduler-k8s-master1                 1/1     Running   0          5m20s

#测试在k8s创建pod是否可以正常访问网络
#把busybox-1-28.tar.gz上传到k8s-node1,node2节点，手动解压
[root@k8s-node1 ~]# docker load -i /root/k8s-yilai/busybox-1-28.tar.gz

kubectl run busybox --image busybox:1.28 --restart=Never --rm -it busybox -- sh
/ # ping www.baidu.com
PING www.baidu.com (39.156.66.18): 56 data bytes
64 bytes from 39.156.66.18: seq=0 ttl=127 time=39.3 ms
#通过上面可以看到能访问网络，说明calico网络插件已经被正常安装了

#  到这里k8s 集群就已经安装完成了

# 配置 补全键
yum install bash-completion -y      # 永久方式 
source /usr/share/bash-completion/bash_completion
cat >> /etc/profile<<EOF
source <(kubectl completion bash)
EOF
source /etc/profile
```





## 二.k8s部署通用组件

### 1.部署nginx-ingress

#### 1.1 创建nginx-ingress-rabc

```shell
[root@k8s-master1 ingress]# cat nginx-ingress-controller-rbac.yml 
#apiVersion: v1
#kind: Namespace
#metadata:  #这里是创建一个namespace，因为此namespace早有了就不用再创建了
#  name: kube-system
---
apiVersion: v1
kind: ServiceAccount    
metadata:
  name: nginx-ingress-serviceaccount #创建一个serveerAcount
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole   #这个ServiceAcount所绑定的集群角色
rules:
  - apiGroups:
      - "" 
    resources:    #此集群角色的权限，它能操作的API资源 
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
        - events
    verbs:
        - create
        - patch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses/status
    verbs:
      - update
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:         
  name: nginx-ingress-role  #这是一个角色，而非集群角色 
  namespace: kube-system
rules:  #角色的权限 
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      # Defaults to "<election-id>-<ingress-class>"
      # Here: "<ingress-controller-leader>-<nginx>"
      # This has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - "ingress-controller-leader-nginx"
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get
      - create
      - update
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding       #角色绑定
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount #绑定在这个用户 
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding      #集群绑定
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount   #集群绑定到这个serviceacount
    namespace: kube-system   #集群角色是可以跨namespace，但是这里只指明给这个namespce来使用

[root@k8s-master1 ingress]# kubectl apply -f nginx-ingress-controller-rbac.yml
```



#### 1.2 创建default-backend

```shell
[root@k8s-master1 ingress]# cat default-backend.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: default-http-backend
  labels:
    k8s-app: default-http-backend
  namespace: kube-system
spec:
  replicas: 1
  selector:
   matchLabels:
     k8s-app: default-http-backend
  template:
    metadata:
      labels:
        k8s-app: default-http-backend
    spec:
      terminationGracePeriodSeconds: 60
      containers:
      - name: default-http-backend
        # Any image is permissable as long as:
        # 1. It serves a 404 page at /
        # 2. It serves 200 on a /healthz endpoint
        image: 192.168.229.21/test/defaultbackend:1.0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          httpGet:
            path: /healthz   #这个URI是 nginx-ingress-controller中nginx里配置好的localtion 
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 30   #30s检测一次/healthz
          timeoutSeconds: 5
        ports:
        - containerPort: 8080
#        resources:
#          limits:
#            cpu: 10m
#            memory: 20Mi
#          requests:
#            cpu: 10m
#            memory: 20Mi
#      nodeName: node1
---
apiVersion: v1
kind: Service     #为default backend 创建一个service
metadata:
  name: default-http-backend
  namespace: kube-system
  labels:
    k8s-app: default-http-backend
spec:
  ports:
  - port: 80
    targetPort: 8080
  selector:
    k8s-app: default-http-backend

[root@k8s-master1 ingress]# kubectl apply -f default-backend.yaml
```

#### 1.3 创建nginx-ingress-controller

```shell
[root@k8s-master1 ingress]# cat nginx-ingress-controller.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  labels:
    k8s-app: nginx-ingress-controller
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
       k8s-app: nginx-ingress-controller
  template:
    metadata:
      labels:
        k8s-app: nginx-ingress-controller
    spec:
      # hostNetwork makes it possible to use ipv6 and to preserve the source IP correctly regardless of docker configuration
      # however, it is not a hard dependency of the nginx-ingress-controller itself and it may cause issues if port 10254 already is taken on the host
      # that said, since hostPort is broken on CNI (https://github.com/kubernetes/kubernetes/issues/31307) we have to use hostNetwork where CNI is used
      # like with kubeadm
      # hostNetwork: true #注释表示不使用宿主机的80口，
      terminationGracePeriodSeconds: 60
      hostNetwork: true  #表示容器使用和宿主机一样的网络
      serviceAccountName: nginx-ingress-serviceaccount #引用前面创建的serviceacount
      containers:   
      - image: 192.168.229.21/test/nginx-ingress-controller:0.20.0      #容器使用的镜像
        imagePullPolicy: IfNotPresent
        name: nginx-ingress-controller  #容器名
        readinessProbe:   #启动这个服务时要验证/healthz 端口10254会在运行的node上监听。 
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10  #每隔10做健康检查 
          timeoutSeconds: 1
        ports:
        - containerPort: 80  
          hostPort: 80    #80映射到80
#        - containerPort: 443
#          hostPort: 443
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        args:
        - /nginx-ingress-controller
        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend
#        - --default-ssl-certificate=$(POD_NAMESPACE)/ingress-secret    #这是启用Https时用的
#      nodeSelector:  #指明运行在哪，此IP要和default backend是同一个IP
#        kubernetes.io/hostname: 10.3.1.17   #上面映射到了hostport80，确保此IP80，443没有占用.
# 
#      nodeName: node1

[root@k8s-master1 ingress]# kubectl apply -f nginx-ingress-controller.yaml

[root@k8s-master1 ingress-0.26]# kubectl get po -n kube-system |grep ingress
nginx-ingress-controller-86d8667cb7-mpndb   1/1     Running   0          29s

```



### 2.部署nfs存储

#### 2.1 部署nfs

```shell
1.部署nfs 动态扩缩容k8s-master1操作
yum install nfs-utils rpcbind -y
systemctl start nfs rpcbind  
systemctl enable nfs rpcbind 
mkdir -p /data/nfs  
chmod 777 /data/nfs
echo "/data/nfs/     192.168.229.0/24(rw,sync,no_root_squash,no_all_squash)" >> /etc/exports

[root@k8s-master1 nfs]# exportfs -arv
exporting 192.168.229.0/24:/data/nfs

[root@k8s-master1 nfs]# showmount -e localhost
参数:
sync：将数据同步写入内存缓冲区与磁盘中，效率低，但可以保证数据的一致性
async：将数据先保存在内存缓冲区中，必要时才写入磁盘

2.所有work节点安装 nfs-utils rpcbind
yum install nfs-utils rpcbind -y
systemctl start nfs  rpcbind
systemctl enable nfs  rpcbind
```



#### 2.2 创建rbac角色

```shell
[root@k8s-master1 nfs]# cat rbac.yaml 
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default   #根据实际环境设定namespace,下面类同
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default  #根据实际环境设定namespace,下面类同
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default   #根据实际环境设定namespace,下面类同
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default    #根据实际环境设定namespace,下面类同
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io

[root@k8s-master1 nfs]# kubectl apply -f rbac.yaml 
serviceaccount/nfs-client-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
```

#### 2.3 创建storageclass 

```shell
[root@k8s-master1 nfs]# cat class.yaml 
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: managed-nfs-storage
provisioner: fuseim.pri/ifs  #这里的名称要和provisioner配置文件中的环境变量PROVISIONER_NAME保持一致
parameters:
  archiveOnDelete: "false"

[root@k8s-master1 nfs]# kubectl apply -f class.yaml 
storageclass.storage.k8s.io/managed-nfs-storage created

```



#### 4.创建provisioner-nfspod

```shell
[root@k8s-master1 nfs]# cat nfs-deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  namespace: default
  labels:
    app: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: 192.168.229.21/test/nfs-subdir-external-provisioner:v4.0.0
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: fuseim.pri/ifs     #provisioner名称,请确保该名称与 nfs-StorageClass.yaml文件中的provisioner名称保持一致
            - name: NFS_SERVER   
              value: 192.168.229.3       #NFS Server IP地址   
            - name: NFS_PATH
              value: /data/nfs          #NFS挂载卷
      nodeName: k8s-node1   # 指定主机
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.229.3    #NFS Server IP地址
            path: /data/nfs     #NFS 挂载卷

[root@k8s-master1 nfs-storageclasses]# kubectl apply -f nfs-deployment.yaml

[root@k8s-master1 nfs-storageclasses]# kubectl get po -n kube-system |grep nfs
nfs-client-provisioner-9599579d7-vzx7j      1/1     Running   0          12s
nfs-client-provisioner-9599579d7-wd6b8      1/1     Running   0          12s

```



### 3.使用heml部署harbor仓库

```shell
[root@k8s-master1 images]# ls
chartmuseum-photon.tar  harbor-jobservice.tar   harbor.txt                redis-photon.tar
harbor-core.tar         harbor-portal.tar       notary-server-photon.tar  registry-photon.tar
harbor-db.tar           harbor-registryctl.tar  notary-signer-photon.tar  trivy-adapter-photon.tar

[root@k8s-master1 images]# cat harbor.txt 
chartmuseum-photon.tar
harbor-core.tar
harbor-db.tar
harbor-jobservice.tar
harbor-portal.tar
harbor-registryctl.tar
notary-server-photon.tar
notary-signer-photon.tar
redis-photon.tar
registry-photon.tar
trivy-adapter-photon.tar

# master节点和node节点都需要全部导入镜像
[root@k8s-master1 images]# for i in `ls ./*.tar`;do docker load -i $i;done

[root@k8s-master1 dev-k8s-ok]# cp helm-3.0/linux-amd64/helm /usr/bin/
[root@k8s-master1 dev-k8s-ok]# chmod +x /usr/bin/helm 


[root@k8s-master1 ingress-0.26]# helm  version
version.BuildInfo{Version:"v3.6.3", GitCommit:"d506314abfb5d21419df8c7e7e68012379db2354", GitTreeState:"clean", GoVersion:"go1.16.5"}

#修改storageclaas 动态创建pv名称
cat harbor-helm-master/values.yaml |grep "storageClass"
# Specify another StorageClass in the "storageClass" or set "existingClaim"
      # Specify the "storageClass" used to provision the volume. Or the default
      storageClass: "managed-nfs-storage"
      storageClass: "managed-nfs-storage"
      storageClass: "managed-nfs-storage"
      storageClass: "managed-nfs-storage"
      storageClass: "managed-nfs-storage"
      storageClass: "managed-nfs-storage"


      
# 创建harbor仓库
[root@k8s-master1 harbor-2.0]# kubectl create ns devops
[root@k8s-master1 harbor-2.0]# helm install paas harbor-helm-master -n devops
# helm uninstall paas -n devops	  # 卸载harbor仓库

# 导出模板命令
helm template  paas  ./harbor-helm-master -f ./harbor-helm-master/values.yaml
```

![image-20220303001306288](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220303001306288.png)



```shell
修改docker配置文件，登录harbor仓库
[root@k8s-master1 mysql]# cat /etc/docker/daemon.json
{
"insecure-registries":["core.harbor.domain"],     # 添加这一行
"registry-mirrors":["https://rsbud4vc.mirror.aliyuncs.com","https://registry.docker-cn.com","https://docker.mirrors.ustc.edu.cn","https://dockerhub.azk8s.cn","http://hub-mirror.c.163.com","http://qtid6917.mirror.aliyuncs.com"], 
"exec-opts":["native.cgroupdriver=systemd"],
 "log-driver":"json-file",
 "log-opts": {
  "max-size": "100m"
  },
 "storage-driver":"overlay2",
 "storage-opts": [
  "overlay2.override_kernel_check=true"
  ]
}

# 刷新配置，重启docker
[root@k8s-node1 images]# systemctl daemon-reload 
[root@k8s-node1 images]# systemctl restart docker

# 查看pod是否都正常启动
[root@k8s-master1 mysql]# kubectl get po -A |grep -v unn

# 配置本地hosts
[root@k8s-master1 mysql]# cat /etc/hosts
192.168.229.4   k8s-node1    core.harbor.domain


# 验证登录harbor仓库
[root@k8s-master1 mysql]# docker login core.harbor.domain
Username: admin
Password: Harbor12345
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded

```

![image-20220303003218934](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220303003218934.png)



在本地配置hosts 对应harbor仓库的ingress

192.168.229.4   core.harbor.domain

Username: admin
Password: Harbor12345![image-20220303001446698](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220303001446698.png)

新建一个项目地址：mogublog

![image-20220303002912308](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220303002912308.png)



### 4.部署sts-MySQL单节点

```shell
1.建立主要工作目录
[root@k8s-master1 ~]# mkdir -p /root/dev-project/mysql   &&  cd /root/dev-project/mysql

2.导入上传MySQL镜像至harbor仓库
[root@k8s-master1 mysql]# docker load -i mysql.tar
[root@k8s-master1 mysql]# docker tag registry.cn-shenzhen.aliyuncs.com/mogublog/mysql:latest   core.harbor.domain/mogublog/mysql:latest
[root@k8s-master1 mysql]# docker push core.harbor.domain/mogublog/mysql:latest

3.编写mysql-sts.yaml文件
[root@k8s-master1 mysql]# cat mysql-dev.yaml 
apiVersion: v1
kind: Service
metadata:
  name: mysql-svc
  namespace: devops
spec:
  type: NodePort
  selector:
    app: mysql
  ports:
   - port: 3306
     targetPort: 3306
     nodePort: 30006
---
apiVersion: v1
kind: Service
metadata:
  name: mysql-headless
  namespace: devops
spec:
  ports:
  - port: 3306
  selector:
    app: mysql
  clusterIP: None
---
apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: StatefulSet
metadata:
  name: mysql
  namespace: devops
spec:
  selector:
    matchLabels:
      app: mysql
  serviceName: "mysql-svc"
  template:
    metadata:
      labels:
        app: mysql # has to match .spec.selector.matchLabels
    spec:
      containers:
      - image: core.harbor.domain/mogublog/mysql:latest
        imagePullPolicy: IfNotPresent
        name: mysql
        env:
          # Use secret in real usage
        - name: MYSQL_ROOT_PASSWORD
          value: "123456"   # mysql初始化密码
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: managed-nfs-storage    # 这边必须和storageClass一致
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: managed-nfs-storage 
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "managed-nfs-storage" # 名字为我们上面创建的StorageClass的名字
      resources:
        requests:
          storage: 5Gi

[root@k8s-master1 mysql]# kubectl apply -f mysql-dev.yaml

[root@k8s-master1 mysql]# kubectl get po -n devops |grep mysql
mysql-0                                     1/1     Running   0          31s
[root@k8s-master1 mysql]# kubectl get svc -n devops |grep mysql
mysql-headless              ClusterIP   None            <none>        3306/TCP            34s
mysql-svc                   NodePort    10.102.7.204    <none>        3306:30006/TCP      34s

4.测试登陆连接数据库
远程连接登陆MySQL数据库
192.168.229.4
30006
root  123456
```

![image-20220303004135617](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220303004135617.png)

```shell
5.创建数据库nacos_config,导入数据库表
进入mysql-pod
# mysql -uroot -p"123456"
mysql> CREATE DATABASE nacos_config CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;
mysql> use nacos_config;
mysql> source /root/nacos-mysql.sql   # 导入nacos数据库
mysql> show tables;
+-------------------------+
| Tables_in_nacos_devtest |
+-------------------------+
| config_info             |
| config_info_aggr        |
| config_info_beta        |
| config_info_tag         |
| config_tags_relation    |
| group_capacity          |
| his_config_info         |
| permissions             |
| roles                   |
| tenant_capacity         |
| tenant_info             |
| users                   |
+-------------------------+

```







### 5.部署sts-nacos单节点服务

```shell
1.导入镜像上传镜像至harbor仓库
[root@k8s-master1 nacos]# docker load -i nacos-server-latest.tar 
[root@k8s-master1 nacos]# docker tag registry.cn-shenzhen.aliyuncs.com/mogublog/nacos-server:latest  core.harbor.domain/mogublog/nacos-server:latest
[root@k8s-master1 nacos]# docker push core.harbor.domain/mogublog/nacos-server:latest

[root@k8s-master1 nacos]# cat nacos-dev.yaml 
---
apiVersion: v1
kind: Service
metadata:
  name: nacos
  namespace: devops
  labels:
    app: nacos
spec:
  type: NodePort
  ports:
    - port: 8848
      name: server
      targetPort: 8848
      nodePort: 30018
    - port: 7848
      name: rpc
      targetPort: 7848
      nodePort: 30019
  selector:
    app: nacos
---
apiVersion: v1
kind: Service
metadata:
  name: nacos-headless
  namespace: devops
  labels:
    app: nacos
spec:
  ports:
    - port: 8848
      name: server
      targetPort: 8848
    - port: 7848
      name: rpc
      targetPort: 7848
  clusterIP: None
  selector:
    app: nacos
---
# 链接mysql数据库
apiVersion: v1
kind: ConfigMap
metadata:
  name: nacos-cm
  namespace: devops
data:
  mysql.db.name: "nacos_config"
  mysql.port: "3306"
  mysql.user: "root"
  mysql.password: "123456"
  
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nacos
  namespace: devops
spec:
  serviceName: nacos-headless
  replicas: 1
  template:
    metadata:
      labels:
        app: nacos
      annotations:
        pod.alpha.kubernetes.io/initialized: "true"
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                      - nacos
              topologyKey: "kubernetes.io/hostname"
      containers:
        - name: nacos
          imagePullPolicy: IfNotPresent
          image: core.harbor.domain/mogublog/nacos-server:latest
          resources:
            requests:
              memory: "2Gi"
              cpu: "500m"
          ports:
            - containerPort: 8848
              name: client-port
            - containerPort: 7848
              name: rpc
          env:
            - name: NACOS_REPLICAS
              value: "1"
            - name: MYSQL_SERVICE_DB_NAME
              valueFrom:
                configMapKeyRef:
                  name: nacos-cm
                  key: mysql.db.name
            - name: MYSQL_SERVICE_PORT
              valueFrom:
                configMapKeyRef:
                  name: nacos-cm
                  key: mysql.port
            - name: MYSQL_SERVICE_USER
              valueFrom:
                configMapKeyRef:
                  name: nacos-cm
                  key: mysql.user
            - name: MYSQL_SERVICE_PASSWORD
              valueFrom:
                configMapKeyRef:
                  name: nacos-cm
                  key: mysql.password
            - name: NACOS_SERVER_PORT
              value: "8848"
            - name: NACOS_APPLICATION_PORT
              value: "8848"
            - name: PREFER_HOST_MODE
              value: "hostname"
            - name: TZ
              value: Asia/Shanghai
            - name: MODE
              value: standalone
          volumeMounts:
            - name: datadir
              mountPath: /home/nacos/data
            - name: logdir
              mountPath: /home/nacos/logs
  volumeClaimTemplates:
    - metadata:
        name: datadir
        annotations:
          volume.beta.kubernetes.io/storage-class: "managed-nfs-storage"
      spec:
        accessModes: [ "ReadWriteMany" ]
        resources:
          requests:
            storage: 3Gi
    - metadata:
        name: logdir
        annotations:
          volume.beta.kubernetes.io/storage-class: "managed-nfs-storage"
      spec:
        accessModes: [ "ReadWriteMany" ]
        resources:
          requests:
            storage: 3Gi
  selector:
    matchLabels:
      app: nacos


[root@k8s-master1 nacos]# kubectl apply -f nacos-dev.yaml 

[root@k8s-master1 nacos]# kubectl get po,svc -n devops |grep nacos
pod/nacos-0                                     1/1     Running   0          4m22s
service/nacos                       NodePort    10.109.70.135   <none>        8848:30018/TCP,7848:30019/TCP   4m22s
service/nacos-headless              ClusterIP   None            <none>        8848/TCP,7848/TCP               4m22s


浏览器访问
http://192.168.229.3:30018/nacos
```

![image-20220303135516446](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220303135516446.png)

创建nacos命名空间

![image-20220303145524293](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220303145524293.png)

![image-20220303145655984](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220303145655984.png)



手动导入nacos配置

![image-20220303160235177](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220303160235177.png)



### 6.部署sts-redis单节点服务

参考文档：[(4条消息) k8s部署redis statefulset应用_学亮编程手记-CSDN博客_k8s部署redis](https://zhangxueliang.blog.csdn.net/article/details/118576012?spm=1001.2101.3001.6650.8&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~HighlightScore-8.queryctrv2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~HighlightScore-8.queryctrv2&utm_relevant_index=14)

```shell
1.上传镜像
[root@k8s-master1 redis]# docker tag busybox:latest  core.harbor.domain/mogublog/busybox:latest
[root@k8s-master1 redis]# docker push core.harbor.domain/mogublog/busybox:latest

[root@k8s-master1 redis]# docker tag redis:5.0.5-alpine    core.harbor.domain/mogublog/redis:5.0.5-alpine
[root@k8s-master1 redis]# docker push core.harbor.domain/mogublog/redis:5.0.5-alpine


[root@k8s-master1 redis]# cat redis-dev.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-conf
  namespace: devops
data:
  redis.conf: |
        bind 0.0.0.0
        port 6379
        requirepass 123456
        pidfile .pid
        appendonly yes
        cluster-config-file nodes-6379.conf
        pidfile /data/middleware-data/redis/log/redis-6379.pid
        cluster-config-file /data/middleware-data/redis/conf/redis.conf
        dir /data/middleware-data/redis/data/
        logfile "/data/middleware-data/redis/log/redis-6379.log"
        cluster-node-timeout 5000
        protected-mode no
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis
  namespace: devops
spec:
  replicas: 1
  serviceName: redis
  selector:
    matchLabels:
      name: redis
  template:
    metadata:
      labels:
        name: redis
    spec:
      initContainers:
      - name: init-redis
        image: busybox:latest
        command: ['sh', '-c', 'mkdir -p /data/middleware-data/redis/log/;mkdir -p /data/middleware-data/redis/conf/;mkdir -p /data/middleware-data/redis/data/']
        volumeMounts:
        - name: data
          mountPath: /data/middleware-data/redis/
      containers:
      - name: redis
        image: redis:5.0.5-alpine
        #image: docker.io/redis:5.0.5-alpine
        imagePullPolicy: IfNotPresent
        command:
        - sh
        - -c
        - "exec redis-server /data/middleware-data/redis/conf/redis.conf"
        ports:
        - containerPort: 6379
          name: redis
          protocol: TCP
        volumeMounts:
        - name: redis-config
          mountPath: /data/middleware-data/redis/conf/
        - name: data
          mountPath: /data/middleware-data/redis/
      volumes:
      - name: redis-config
        configMap:
          name: redis-conf

  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "managed-nfs-storage" # 名字为我们上面创建的StorageClass的名字
      resources:
        requests:
          storage: 3Gi
---
# 内部访问的无头服务
apiVersion: v1
kind: Service
metadata:
  labels:
    name: redis
  name: redis-service
  namespace: devops
spec:
  selector:
    app: redis
  ports:
  - name: redis
    port: 6379
  clusterIP: None
---
kind: Service
apiVersion: v1
metadata:
  labels:
    name: redis
  name: redis
  namespace: devops
spec:
  type: NodePort
  ports:
  - name: redis
    port: 6379
    targetPort: 6379
    nodePort: 31379
  selector:
    name: redis    

[root@k8s-master1 redis]# kubectl aaply -f redis-dev.yaml 
persistentvolumeclaim/redis-data-redis-0                     Bound    pvc-4338d703-b71b-4596-a866-3bc0fd7363f5   5Gi        RWO            managed-nfs-storage   18h
[root@k8s-master1 redis]# kubectl get po -n devops |grep redis
redis-0                                     1/1     Running   0          3m38s

192.168.229.3
端口：31379
密码：123456
```



### 7.部署sts-rabbitmq单节点服务

[kubernetes-部署RabbitMQ_Kubernetes中文社区](https://www.kubernetes.org.cn/6580.html)

```shell
docker pull rabbitmq:3.7-rc-management
docker tag  rabbitmq:3.7-rc-management    core.harbor.domain/test/rabbitmq:3.7-rc-management
docker push core.harbor.domain/test/rabbitmq:3.7-rc-management

[root@k8s-master1 rabbitmq]# cat rabbitmq-dev.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: rabbitmq
  namespace: devops
spec:
  selector:
    matchLabels:
      app: rabbitmq
  serviceName: "rabbitmq-service"
  replicas: 1
  template:
    metadata:
      labels:
        app: rabbitmq
    spec:
      terminationGracePeriodSeconds: 10
      nodeName: k8s-master1
      containers:
      - name: rabbitmq
        image: core.harbor.domain/test/rabbitmq:3.7-rc-management
        imagePullPolicy: IfNotPresent
        env:
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: RABBITMQ_ERLANG_COOKIE
          value: "YZSDHWMFSMKEMBDHSGGZ"
        - name: RABBITMQ_NODENAME
          value: "rabbit@$(MY_POD_NAME)"
        ports:
        - containerPort: 15672
          name: rabbit15672
          protocol: TCP
        - containerPort: 5672 
          name: rabbit5672 
          protocol: TCP
        volumeMounts:
        - name: rabbitmq-data
          mountPath: /var/lib/rabbitmq
  volumeClaimTemplates:
  - metadata:
      name: rabbitmq-data
      annotations:
        volume.beta.kubernetes.io/storage-class: managed-nfs-storage
    spec:
      accessModes: [ "ReadWriteMany" ]
      resources:
        requests:
          storage: 1Gi
---
apiVersion: v1
kind: Service
metadata:
  name: rabbitmq-service
  namespace: devops
spec:
  type: NodePort
  ports:
  - name: rabbitmqtcp
    port: 5672
    targetPort: 5672
    nodePort: 30001
  - name: managent
    port: 15672
    targetPort: 15672
    nodePort: 30002
  selector:
    app: rabbitmq

[root@k8s-master1 rabbitmq]# kubectl apply -f rabbitmq-dev.yaml

[root@k8s-master1 rabbitmq]# kubectl get po,pvc,svc -n devops |grep rabbitmq
pod/rabbitmq-0                                  1/1     Running   0          3m35s
persistentvolumeclaim/rabbitmq-data-rabbitmq-0               Bound    pvc-7c9433f2-2b45-44bd-b7cb-66c316bd043f   1Gi        RWX            managed-nfs-storage   5m59s
persistentvolumeclaim/rabbitmqdata-rabbitmq-0                Bound    pvc-daf05e27-c9c8-405f-b5f2-a1d405cc2417   1Gi        RWX            managed-nfs-storage   154m
service/rabbitmq-service            NodePort    10.109.181.185   <none>        5672:30001/TCP,15672:30002/TCP   3m35s

浏览器访问：
http://192.168.229.3:30002
默认账号密码：guest   guest
```

![image-20220304142724046](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220304142724046.png)



### 8.部署zipkin组件

[(4条消息) K8S集群部署链路追踪——Zipkin_jiang_shikui的博客-CSDN博客_k8s 链路追踪](https://blog.csdn.net/jiang_shikui/article/details/84946586)

```shell
docker pull openzipkin/zipkin
[root@k8s-master1 zipkin]# cat zipkin-dev.yaml 
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zipkin
  namespace: devops
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zipkin
  template:
   metadata:
    labels:
     app: zipkin
   spec:
    nodeName: k8s-master1
    containers:
     - name: zipkin
       image: openzipkin/zipkin
       imagePullPolicy: IfNotPresent
       resources:
         limits:
           cpu: 150m
           memory: 256Mi
         requests:
           cpu: 100m
           memory: 128Mi
       env:
         - name: TZ
           value: Asia/Shanghai
       ports:
         - containerPort: 9411
---
apiVersion: v1
kind: Service
metadata:
  name: zipkin
  namespace: devops
spec:
  type: NodePort
  ports:
   - port: 80
     targetPort: 9411
     nodePort: 30095
  selector:
    app: zipkin

[root@k8s-master1 zipkin]# kubectl apply -f zipkin-dev.yaml
[root@k8s-master1 zipkin]# kubectl get po,svc -n devops |grep zipkin
pod/zipkin-6fc47b74df-5hzpf                     1/1     Running   0          4m19s
service/zipkin                      NodePort    10.106.58.232    <none>        80:30095/TCP                     4m19s


浏览器访问：
http://192.168.229.3:30095
```

![image-20220304145005338](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220304145005338.png)



### 9.部署sts-sentinel组件

[k8s：部署sentinel-dashboard - 简书 (jianshu.com)](https://www.jianshu.com/p/78c50c8b5918)

```shell
[root@k8s-master1 sentinel]# cat sentinel-dev.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: sentinel
spec:
  serviceName: sentinel
  replicas: 1
  selector:
    matchLabels:
      app: sentinel
  template:
    metadata:
      labels:
        app: sentinel
      annotations:
        pod.alpha.kubernetes.io/initialized: "true"
    spec:
      nodeName: k8s-master1
      containers:
        - name: sentinel
          imagePullPolicy: IfNotPresent
          image: bladex/sentinel-dashboard
          ports:
            - containerPort: 8858
              name: client
          env:
            - name: TZ
              value: Asia/Shanghai
            - name: JAVA_OPTS
              value: "-Dserver.port=8858 -Dcsp.sentinel.dashboard.server=localhost:8858 -Dsentinel.dashboard.auth.username=sentinel -Dsentinel.dashboard.auth.password=sentinel -Dserver.servlet.session.timeout=7200"
---
apiVersion: v1
kind: Service
metadata:
  name: sentinel-svc
  labels:
    app: sentinel
spec:
  ports:
    - protocol: TCP
      name: http
      port: 8858
      targetPort: 8858
      nodePort: 31808
  type: NodePort
  selector:
    app: sentinel


# 访问Sentinel登录页面，输入账号密码： sentinel  sentinel
http://192.168.229.3:31808
```



![image-20220304151144778](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220304151144778.png)



### 10.部署sts-elasticsearch组件

```shell
[root@k8s-master1 elasticsearch]# dcoker load -i images/elasticsearch_7_2_0.tar.gz
[root@k8s-master1 images]# docker tag docker.elastic.co/elasticsearch/elasticsearch:7.2.0  core.harbor.domain/mogublog/elasticsearch:7.2.0
[root@k8s-master1 images]# docker push core.harbor.domain/mogublog/elasticsearch:7.2.0


[root@k8s-master1 elasticsearch]# cat elasticsearch-dev.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-cluster
  namespace: devops
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      nodeName: k8s-master1
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0
        imagePullPolicy: IfNotPresent
        resources:
            limits:
              cpu: 1000m
            requests:
              cpu: 100m
        ports:
        - containerPort: 9200
          name: rest
          protocol: TCP
        - containerPort: 9300
          name: inter-node
          protocol: TCP
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        env:
          - name: cluster.name
            value: k8s-logs
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: discovery.seed_hosts
            value: "es-cluster-0.elasticsearch.kube-logging.svc.cluster.local,es-cluster-1.elasticsearch.kube-logging.svc.cluster.local,es-cluster-2.elasticsearch.kube-logging.svc.cluster.local"
          - name: cluster.initial_master_nodes
            value: "es-cluster-0,es-cluster-1,es-cluster-2"
          - name: ES_JAVA_OPTS
            value: "-Xms512m -Xmx512m"
      initContainers:
      - name: fix-permissions
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c", "chown -R 1000:1000 /usr/share/elasticsearch/data"]
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      - name: increase-vm-max-map
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        securityContext:
          privileged: true
      - name: increase-fd-ulimit
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c", "ulimit -n 65536"]
        securityContext:
          privileged: true
  volumeClaimTemplates:
  - metadata:
      name: data
      labels:
        app: elasticsearch
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: managed-nfs-storage
      resources:
        requests:
          storage: 3Gi

---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: devops
  labels:
    app: elasticsearch
spec:
  type: NodePort
  ports:
    - port: 9200
      targetPort: 9200
      nodePort: 30920
      name: es-9200
    - port: 9300
      targetPort: 9300
      nodePort: 30930
      name: es-9300
  selector:
    app: elasticsearch
    
[root@k8s-master1 elasticsearch]# kubectl apply -f  elasticsearch-dev.yaml 
# 为了减小内存使用可以将es缩容为一个
[root@k8s-master1 elasticsearch]# kubectl scale sts es-cluster --replicas=1

[root@k8s-master1 elasticsearch]# kubectl get po,svc,pvc -n devops |grep es
pod/es-cluster-0                                1/1     Running   0          9m3s
service/mysql-headless              ClusterIP   None             <none>        3306/TCP                         39h
service/nacos-headless              ClusterIP   None             <none>        8848/TCP,7848/TCP                26h
persistentvolumeclaim/data-es-cluster-0                      Bound    pvc-fe855ff9-ec84-4ee8-96f1-06e56b2033a1   3Gi        RWO            managed-nfs-storage   9m3s
persistentvolumeclaim/data-es-cluster-1                      Bound    pvc-accdff19-c827-48a3-b2e9-2fe66e2cc5c0   3Gi        RWO            managed-nfs-storage   8m59s
persistentvolumeclaim/data-es-cluster-2                      Bound    pvc-9bbd7b71-251b-4c69-aa8c-fcb42b77a424   3Gi        RWO            managed-nfs-storage   8m53s

浏览器访问：
http://192.168.229.3:30920/
```

![image-20220304155924951](%E8%98%91%E8%8F%87%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.assets/image-20220304155924951.png)





















